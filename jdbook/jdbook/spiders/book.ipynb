{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from scrapy_redis.spiders import RedisSpider\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "class BookSpider(RedisSpider):\n",
    "    name = 'book'\n",
    "    allowed_domains = ['jd.com', 'p.3.cn']\n",
    "\n",
    "    redis_key = 'jd'\n",
    " #   start_urls = ['https://book.jd.com/booksort.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        dt_list = response.xpath('//div[@class=\"mc\"]/dl/dt')\n",
    "\n",
    "        for dt in dt_list:\n",
    "            item = {}\n",
    "\n",
    "            item['b_cate'] = dt.xpath('./a/text()').extract_first()\n",
    "\n",
    "            em_list = dt.xpath('./following-sibling::dd[1]/em')\n",
    "\n",
    "            for em in em_list:\n",
    "                item['s_href'] = em.xpath('./a/@href').extract_first()\n",
    "                item['s_cate'] = em.xpath('./a/text()').extract_first()\n",
    "\n",
    "                if item['s_href'] is not None:\n",
    "                    yield scrapy.Request('https:'+item['s_href'],\n",
    "                                         callback = self.parseBook,\n",
    "                                         meta= {'item': deepcopy(item)})\n",
    "\n",
    "\n",
    "    def parseBook(self, response):\n",
    "\n",
    "        item = response.meta['item']\n",
    "        li_list = response.xpath('//*[@id=\"plist\"]/ul/li')\n",
    "        base_url = 'https://list.jd.com'\n",
    "\n",
    "        for li in li_list:\n",
    "            item['book_img'] = li.xpath('.//div[@class=\"p-img\"]//img/@src').extract_first()\n",
    "            if item['book_img'] is None:\n",
    "                item['book_img'] = li.xpath('.//div[@class=\"p-img\"]//img/@data-lazy-img').extract_first()\n",
    "\n",
    "            item['book_name'] = li.xpath('.//div[@class=\"p-name\"]//em/text()').extract_first().strip()\n",
    "            item['book_author'] = '/'.join(li.xpath('.//span[@class=\"p-bi-name\"]/span/a/text()').extract())\n",
    "            item['book_pub'] = li.xpath('.//span[@class=\"p-bi-store\"]/a/@title').extract_first()\n",
    "            item['pub_date'] = li.xpath('.//span[@class=\"p-bi-date\"]/text()').extract_first().strip()\n",
    "\n",
    "            sku = li.xpath('./div/@data-sku').extract_first()\n",
    "            price_url = 'https://p.3.cn/prices/mgets?skuIds=J_{}'.format(sku)\n",
    "\n",
    "            yield scrapy.Request(price_url, callback=self.parsePrice, meta={'item': item})\n",
    "\n",
    "        next_page = response.xpath('//a[@class=\"pn-next\"]/@href').extract_first()\n",
    "        if next_page is not None:\n",
    "            yield scrapy.Request(base_url + next_page,\n",
    "                                 callback=self.parseBook,\n",
    "                                 meta={'item': item}\n",
    "                                 )\n",
    "\n",
    "    def parsePrice(self, response):\n",
    "        item = response.meta['item']\n",
    "        json_dic = json.loads(response.text)\n",
    "\n",
    "        if isinstance(json_dic, list):\n",
    "            item['book_price'] = json_dic[0].get('op', 0)\n",
    "        else:\n",
    "            item['book_price'] = 0\n",
    "\n",
    "        yield item\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
